# stock-data-pipeline

This project implements an end-to-end data pipeline that retrieves daily stock market data from the Alpha Vantage API, transforms it using Python, stores it in a PostgreSQL database hosted on AWS RDS, and visualizes key financial metrics using Grafana. The entire workflow is orchestrated and automated with Apache Airflow.

## Architecture Diagram

```mermaid
flowchart TB

    %% Data Acquisition
    subgraph DA["ðƒðšð­ðš ð€ðœðªð®ð¢ð¬ð¢ð­ð¢ð¨ð§"]
        direction TB
        API["ð€ð¥ð©ð¡ðš ð•ðšð§ð­ðšð ðž ð€ððˆ"]
        FSD["ð…ðžð­ðœð¡_ð¬ð­ð¨ðœð¤_ððšð­ðš.ð©ð²"]
        ENV[".ðžð§ð¯ ðŸð¢ð¥ðž"]
    end

    %% Data Transformation
    subgraph DT["ðƒðšð­ðš ð“ð«ðšð§ð¬ðŸð¨ð«ð¦ðšð­ð¢ð¨ð§"]
        direction TB
        CAT["ðœð¥ðžðšð§_ðšð§ð_ð­ð«ðšð§ð¬ðŸð¨ð«ð¦_ððšð­ðš.ð©ð²"]
    end

    %% Data Storage
    subgraph DS["ðƒðšð­ðš ð’ð­ð¨ð«ðšð ðž"]
        direction TB
        AWS["ð€ð–ð’ ð‘ðƒð’ ðð¨ð¬ð­ð ð«ðžð’ðð‹"]
        T1["ððšð¢ð¥ð²_ð¬ð­ð¨ðœð¤_ððšð­ðš"]
        T2["ð­ð«ðšð§ð¬ðŸð¨ð«ð¦ðžð_ð¬ð­ð¨ðœð¤_ððšð­ðš"]
        PG["ð©ð ð€ðð¦ð¢ð§"]
    end

    %% Orchestration
    subgraph ORC["ðŽð«ðœð¡ðžð¬ð­ð«ðšð­ð¢ð¨ð§"]
        direction TB
        AF["ð€ð©ðšðœð¡ðž ð€ð¢ð«ðŸð¥ð¨ð°"]
        DAG["ð¬ð­ð¨ðœð¤_ððšð­ðš_ð©ð¢ð©ðžð¥ð¢ð§ðž_ððšð .ð©ð²"]
        FT["ðŸðžð­ðœð¡_ð¬ð­ð¨ðœð¤_ððšð­ðš_ð­ðšð¬ð¤"]
        TT["ð­ð«ðšð§ð¬ðŸð¨ð«ð¦_ðšð§ð_ð¥ð¨ðšð_ð­ðšð¬ð¤"]
    end

    %% Visualization
    subgraph VIS["ð•ð¢ð¬ð®ðšð¥ð¢ð³ðšð­ð¢ð¨ð§"]
        direction TB
        GF["ð†ð«ðšðŸðšð§ðš ððšð¬ð¡ð›ð¨ðšð«ðð¬"]
        GFDB["ðð¨ð¬ð­ð ð«ðžð’ðð‹ (ð¬ð­ð¨ðœð¤_ððšð­ðš_ðð›)"]
    end

    %% Connections
    API --> FSD
    FSD --> T1
    ENV --- FSD

    T1 --> CAT
    CAT --> T2

    AWS --- PG

    T2 --- GFDB
    GFDB --> GF

    AF --> DAG
    DAG --> FT
    DAG --> TT
    FT --> FSD
    TT --> CAT

    %% Styling
    style API fill:#f9f,stroke:#333,stroke-width:2px,color:#333
    style FSD fill:#bbf,stroke:#333,stroke-width:2px,color:#333
    style ENV fill:#ddd,stroke:#333,stroke-width:1px,color:#333
    style CAT fill:#bbf,stroke:#333,stroke-width:2px,color:#333
    style AWS fill:#9f9,stroke:#333,stroke-width:2px,color:#333
    style T1 fill:#cde,stroke:#333,stroke-width:1px,color:#333
    style T2 fill:#cde,stroke:#333,stroke-width:1px,color:#333
    style PG fill:#e6e6fa,stroke:#333,stroke-width:1px,color:#333
    style AF fill:#f8d568,stroke:#333,stroke-width:2px,color:#333
    style DAG fill:#f8d568,stroke:#333,stroke-width:1px,color:#333,stroke-dasharray: 5 5
    style FT fill:#f8d568,stroke:#333,stroke-width:1px,color:#333
    style TT fill:#f8d568,stroke:#333,stroke-width:1px,color:#333
    style GF fill:#ffb366,stroke:#333,stroke-width:2px,color:#333
    style GFDB fill:#ffcc99,stroke:#333,stroke-width:1px,color:#333

```

## Project Overview

The primary goal of this project is to build an automated system for collecting, processing, storing, and analyzing stock market data for multiple symbols. This allows for tracking price trends, moving averages, and other potential Key Performance Indicators (KPIs).

### Key Features:

1. **Data Fetching:** Fetches daily time series data (closing price, open, high, low, volume) for user-specified stock symbols (e.g., IBM, AAPL, MSFT) from the Alpha Vantage API.
2. **Data Transformation:** Calculates key metrics such as the 20-day Moving Average (MA) and daily percentage change for fetched stock data.
3. **Data Storage:** Stores both raw and transformed stock data in a PostgreSQL database hosted on **AWS RDS**.
4. **Orchestration:** An **Apache Airflow DAG (`stock_data_pipeline`)** manages the data fetching, transformation, and loading tasks, allowing for parameterized runs (by stock symbol) and configured retries.
5. **Data Visualization:** A **Grafana dashboard** connects to the AWS RDS database to display interactive charts, currently showing closing prices and 20-day moving averages for selected stocks.

## Technologies Used

- **Python 3**
- **Pandas**
- **Requests**
- **Psycopg2**
- **python-dotenv**
- **PostgreSQL**
- **AWS RDS**
- **Apache Airflow**
- **Grafana**
- **Git & GitHub**

## Project Structure

```
stock-data-pipeline/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ stock_data_pipeline_dag.py
â”œâ”€â”€ Workspace_stock_data.py
â”œâ”€â”€ clean_and_transform_data.py
â”œâ”€â”€ Workspace_and_analyze_stockdata.py
â”œâ”€â”€ grafana_dashboards/
â”‚   â””â”€â”€ dashboard_export.json (optional)
â”œâ”€â”€ images/
â”‚   â””â”€â”€screenshot.png
â””â”€â”€ README.md
```

## Setup and Installation

### 1. Clone the Repository

```bash
git clone https://github.com/YOUR_USERNAME/stock-data-pipeline.git
cd stock-data-pipeline
```

### 2. Create Virtual Environment and Install Dependencies

```bash
python3 -m venv venv
source venv/bin/activate
pip install pandas requests psycopg2-binary python-dotenv argparse
```

### 3. AWS RDS PostgreSQL Setup

- Create an RDS PostgreSQL instance.
- Ensure it's publicly accessible and port 5432 is open.
- Create a database (e.g., `stock_data_db`).

Example SQL for table setup:

```sql
-- Raw data table
CREATE TABLE IF NOT EXISTS daily_stock_data (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    trade_date DATE NOT NULL,
    open_price NUMERIC(10, 2),
    high_price NUMERIC(10, 2),
    low_price NUMERIC(10, 2),
    close_price NUMERIC(10, 2),
    volume BIGINT,
    fetched_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (symbol, trade_date)
);

-- Transformed data table
CREATE TABLE IF NOT EXISTS transformed_stock_data (
    id SERIAL PRIMARY KEY,
    symbol VARCHAR(10) NOT NULL,
    trade_date DATE NOT NULL,
    open_price NUMERIC(10,2),
    high_price NUMERIC(10,2),
    low_price NUMERIC(10,2),
    close_price NUMERIC(10,2),
    volume BIGINT,
    ma_20_day NUMERIC(10,2),
    daily_pct_change NUMERIC(10,4),
    transformed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE (symbol, trade_date)
);
```

### 4. Create `.env` File

```env
ALPHA_VANTAGE_API_KEY="your_api_key"
DB_HOST="your-db-host.rds.amazonaws.com"
DB_NAME="stock_data_db"
DB_USER="your_db_username"
DB_PASSWORD_POSTGRES="your_db_password"
```

Ensure `.env` is in your `.gitignore`.

### 5. Set Up Apache Airflow

- Install Airflow in a separate virtual environment.
- Run:

```bash
pip install apache-airflow
airflow db migrate
```

- Put your DAG file in `~/airflow/dags` or link it:

```bash
ln -s /path/to/stock-data-pipeline/dags/stock_data_pipeline_dag.py ~/airflow/dags/
```

### 6. Set Up Grafana

- Install Grafana (e.g., via Homebrew or official package).
- Start Grafana: `brew services start grafana`
- Visit: [http://localhost:3000](http://localhost:3000)
- Add your PostgreSQL RDS as a data source.

## Running the Pipeline

### 1. Start Airflow

```bash
source ~/airflow_venv/bin/activate
airflow standalone
```

- Open Airflow UI at `http://localhost:8080`
- Unpause and trigger the `stock_data_pipeline` DAG.

### 2. Open Grafana Dashboard

- Go to `http://localhost:3000`
- View your dashboard (e.g., "Stock Price Dashboard")

## Dashboard Preview

![Grafana Dashboard](images/screenshot.png)

(Optional: Import `grafana_dashboards/dashboard_export.json` in Grafana to reproduce this layout.)

## Current Status

- End-to-end pipeline: fetch â†’ transform â†’ store
- Orchestration with Apache Airflow
- Visualization with Grafana
- Secrets managed with `.env`

## Read the Full Walkthrough

I wrote a short article describing how I built this project and why I made certain decisions.

ðŸ‘‰ [How I Built a Stock Data Pipeline](https://liinda.hashnode.dev/how-i-built-a-stock-data-pipeline)


## Next Steps

- Add more KPIs (e.g., % change, volume trends)
- Data validation & anomaly detection
- Notifications on DAG failure
- Modularize ETL scripts
- Deploy to the cloud (e.g., EC2, ECS, or Lambda + Step Functions)
